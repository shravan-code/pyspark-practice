{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7c98835",
   "metadata": {},
   "source": [
    "Perfect. Continuing in the **same interview-ready depth and rigor**.\n",
    "\n",
    "---\n",
    "\n",
    "# âœ… PySpark Top 100 Methods â€” **PART 2 (26â€“50)**\n",
    "\n",
    "**Category: Aggregations, GroupBy, Joins, Set Operations**\n",
    "\n",
    "These methods dominate **mid-level and senior data engineer interviews**.\n",
    "\n",
    "---\n",
    "\n",
    "## 2ï¸âƒ£6ï¸âƒ£ `df.groupBy()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Groups rows based on column(s).\n",
    "\n",
    "### **Why**\n",
    "\n",
    "Required for aggregations.\n",
    "\n",
    "### **How**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e1dea1",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "df.groupBy(\"dept\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c86ef47",
   "metadata": {},
   "source": [
    "### **Interview**\n",
    "\n",
    "> `groupBy()` alone does nothing â€” needs an aggregation\n",
    "\n",
    "---\n",
    "\n",
    "## 2ï¸âƒ£7ï¸âƒ£ `agg()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Applies aggregate functions.\n",
    "\n",
    "### **Why**\n",
    "\n",
    "Multiple aggregations in one pass.\n",
    "\n",
    "### **How**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5551204",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, max\n",
    "df.groupBy(\"dept\").agg(\n",
    "    avg(\"salary\"),\n",
    "    max(\"salary\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32907ea5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2ï¸âƒ£8ï¸âƒ£ `count()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Counts rows per group.\n",
    "\n",
    "### **How**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc07d905",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "df.groupBy(\"dept\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d19ee33",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2ï¸âƒ£9ï¸âƒ£ `sum()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Computes sum.\n",
    "\n",
    "### **How**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaae79d",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "df.groupBy(\"dept\").sum(\"salary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6171c7d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3ï¸âƒ£0ï¸âƒ£ `avg()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Average aggregation.\n",
    "\n",
    "### **Interview**\n",
    "\n",
    "> Uses **double precision** internally\n",
    "\n",
    "---\n",
    "\n",
    "## 3ï¸âƒ£1ï¸âƒ£ `min()` / `max()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Minimum / maximum value.\n",
    "\n",
    "### **How**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bab45b",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "df.groupBy(\"dept\").max(\"salary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfe170d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3ï¸âƒ£2ï¸âƒ£ `countDistinct()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Counts unique values.\n",
    "\n",
    "### **How**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9868a06",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "df.select(countDistinct(\"user_id\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef6f186",
   "metadata": {},
   "source": [
    "### **Interview**\n",
    "\n",
    "> Expensive â†’ requires shuffle\n",
    "\n",
    "---\n",
    "\n",
    "## 3ï¸âƒ£3ï¸âƒ£ `approx_count_distinct()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Approximate distinct count.\n",
    "\n",
    "### **Why**\n",
    "\n",
    "Massive performance gain.\n",
    "\n",
    "### **How**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839327a8",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import approx_count_distinct\n",
    "df.select(approx_count_distinct(\"user_id\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16055fe1",
   "metadata": {},
   "source": [
    "### **Interview**\n",
    "\n",
    "> Uses **HyperLogLog++**\n",
    "\n",
    "---\n",
    "\n",
    "## 3ï¸âƒ£4ï¸âƒ£ `pivot()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Converts rows â†’ columns.\n",
    "\n",
    "### **How**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23674be",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "df.groupBy(\"dept\").pivot(\"year\").sum(\"salary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfa4c11",
   "metadata": {},
   "source": [
    "### **Use Case**\n",
    "\n",
    "* Reports\n",
    "* BI transformations\n",
    "\n",
    "---\n",
    "\n",
    "## 3ï¸âƒ£5ï¸âƒ£ `join()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Combines DataFrames.\n",
    "\n",
    "### **How**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda02ab9",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "df1.join(df2, on=\"id\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ec85a2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3ï¸âƒ£6ï¸âƒ£ Join Types\n",
    "\n",
    "| Type  | Use             |\n",
    "| ----- | --------------- |\n",
    "| inner | Matching rows   |\n",
    "| left  | All left rows   |\n",
    "| right | All right rows  |\n",
    "| full  | All rows        |\n",
    "| semi  | Exists in right |\n",
    "| anti  | Not exists      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0850cac",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "df1.join(df2, \"id\", \"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bfdb1b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3ï¸âƒ£7ï¸âƒ£ `broadcast()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Broadcasts small table.\n",
    "\n",
    "### **Why**\n",
    "\n",
    "Avoids shuffle.\n",
    "\n",
    "### **How**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d712c07",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "df_large.join(broadcast(df_small), \"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f2c725",
   "metadata": {},
   "source": [
    "### **Interview Gold**\n",
    "\n",
    "> Broadcast if **< 10â€“50 MB**\n",
    "\n",
    "---\n",
    "\n",
    "## 3ï¸âƒ£8ï¸âƒ£ `crossJoin()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Cartesian product.\n",
    "\n",
    "### **Danger**\n",
    "\n",
    "âŒ Extremely expensive.\n",
    "\n",
    "---\n",
    "\n",
    "## 3ï¸âƒ£9ï¸âƒ£ `union()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Row-wise union.\n",
    "\n",
    "### **How**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0004c9",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "df1.union(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a46dd83",
   "metadata": {},
   "source": [
    "### **Requirement**\n",
    "\n",
    "> Same schema order\n",
    "\n",
    "---\n",
    "\n",
    "## 4ï¸âƒ£0ï¸âƒ£ `unionByName()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Union by column name.\n",
    "\n",
    "### **Why**\n",
    "\n",
    "Schema mismatch safe.\n",
    "\n",
    "### **How**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea9e675",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "df1.unionByName(df2, allowMissingColumns=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa59da63",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4ï¸âƒ£1ï¸âƒ£ `intersect()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Common rows.\n",
    "\n",
    "### **Cost**\n",
    "\n",
    "âš ï¸ Shuffle required.\n",
    "\n",
    "---\n",
    "\n",
    "## 4ï¸âƒ£2ï¸âƒ£ `exceptAll()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Rows in DF1 not in DF2.\n",
    "\n",
    "---\n",
    "\n",
    "## 4ï¸âƒ£3ï¸âƒ£ `having` (via filter)\n",
    "\n",
    "### **What**\n",
    "\n",
    "Filter after aggregation.\n",
    "\n",
    "### **How**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d221a1",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "df.groupBy(\"dept\").count().filter(\"count > 10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29f5edf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4ï¸âƒ£4ï¸âƒ£ `groupingSets()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Multiple groupings.\n",
    "\n",
    "### **Why**\n",
    "\n",
    "Advanced analytics.\n",
    "\n",
    "---\n",
    "\n",
    "## 4ï¸âƒ£5ï¸âƒ£ `rollup()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Hierarchical aggregation.\n",
    "\n",
    "### **How**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfc5515",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "df.rollup(\"country\", \"state\").sum(\"sales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2465aa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4ï¸âƒ£6ï¸âƒ£ `cube()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "All combinations aggregation.\n",
    "\n",
    "### **Interview**\n",
    "\n",
    "> More expensive than `rollup`\n",
    "\n",
    "---\n",
    "\n",
    "## 4ï¸âƒ£7ï¸âƒ£ `repartition()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Changes partition count (shuffle).\n",
    "\n",
    "### **How**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc869752",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "df.repartition(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2048bcb3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4ï¸âƒ£8ï¸âƒ£ `coalesce()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Reduce partitions (no shuffle).\n",
    "\n",
    "### **How**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e40ed87",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "df.coalesce(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445217b7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4ï¸âƒ£9ï¸âƒ£ `explain()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Execution plan.\n",
    "\n",
    "### **How**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f089fd4",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "df.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e491fc",
   "metadata": {},
   "source": [
    "### **Interview**\n",
    "\n",
    "> Know **Logical vs Physical Plan**\n",
    "\n",
    "---\n",
    "\n",
    "## 5ï¸âƒ£0ï¸âƒ£ `cache()` / `persist()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Stores DF in memory.\n",
    "\n",
    "### **How**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998bd298",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a0d254",
   "metadata": {},
   "source": [
    "### **Interview**\n",
    "\n",
    "> Use before **multiple actions**\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ”¥ Interview Coverage (Part 2)\n",
    "\n",
    "âœ” Aggregations\n",
    "âœ” Joins (broadcast vs shuffle)\n",
    "âœ” Set operations\n",
    "âœ” Performance tuning\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ‘‰ Next:\n",
    "\n",
    "**PART 3 (51â€“75):**\n",
    "\n",
    "* Window functions\n",
    "* UDFs\n",
    "* JSON / Date / Array / Map functions\n",
    "* explode, collect_list\n",
    "* SQL functions vs DataFrame API\n",
    "\n",
    "Say **â€œContinue Part 3â€**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
