{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "473f46d3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# âœ… PySpark Top 100 Methods â€” **PART 3 (51â€“75)**\n",
    "\n",
    "**Category: Window Functions, Built-in Functions, UDFs, Complex Types, Dates, JSON**\n",
    "\n",
    "These are **senior-level differentiators** in PySpark interviews.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a35b6108",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit\n",
    "from pyspark.sql.types import StructType, StringType, StructField, IntegerType, FloatType, DoubleType, DecimalType, DatetimeType, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e579020e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+---------+--------------------+\n",
      "|song_id| title|artist_id|        release_date|\n",
      "+-------+------+---------+--------------------+\n",
      "|      1|Song_1|        2|2021-10-15 10:15:...|\n",
      "|      2|Song_2|       45|2020-12-07 10:15:...|\n",
      "|      3|Song_3|       25|2022-07-11 10:15:...|\n",
      "|      4|Song_4|       25|2019-03-09 10:15:...|\n",
      "|      5|Song_5|       26|2019-09-07 10:15:...|\n",
      "+-------+------+---------+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "songs_schema = StructType([\n",
    "    StructField(\"song_id\", IntegerType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"artist_id\", IntegerType(), True),\n",
    "    StructField(\"release_date\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "filepath = str(pathlib.Path().cwd().parent / \"data\" / \"Spotify_songs.csv\")\n",
    "\n",
    "sparksession = SparkSession.builder.appName(\"Spotify_App\").getOrCreate()\n",
    "\n",
    "songs_df = sparksession.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"delimiter\", \",\") \\\n",
    "    .option(\"mode\", \"PERMISSIVE\") \\\n",
    "    .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\") \\\n",
    "    .schema(songs_schema) \\\n",
    "    .csv(filepath)\n",
    "    \n",
    "songs_df.show(5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16699e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+--------------------+---------------+\n",
      "|activity_id|song_id|         listen_date|listen_duration|\n",
      "+-----------+-------+--------------------+---------------+\n",
      "|          1|     12|2023-06-27 10:15:...|             69|\n",
      "|          2|     44|2023-06-27 10:15:...|            300|\n",
      "|          3|     75|2023-06-27 10:15:...|             73|\n",
      "|          4|     48|2023-06-27 10:15:...|            105|\n",
      "|          5|     10|2023-06-27 10:15:...|            229|\n",
      "+-----------+-------+--------------------+---------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "listening_schema = StructType([\n",
    "    StructField(\"activity_id\", IntegerType(), True),\n",
    "    StructField(\"song_id\", IntegerType(), True),\n",
    "    StructField(\"listen_date\", TimestampType(), True),\n",
    "    StructField(\"listen_duration\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "filepath = str(pathlib.Path().cwd().parent / \"data\" / \"Spotify_Listening_Activity.csv\")\n",
    "\n",
    "listening_df = sparksession.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"delimiter\", \",\") \\\n",
    "    .option(\"mode\", \"PERMISSIVE\") \\\n",
    "    .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\") \\\n",
    "    .schema(listening_schema) \\\n",
    "    .csv(filepath)\n",
    "    \n",
    "listening_df.show(5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04502207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+----------+---------+\n",
      "|artist_id|    name|     genre|  country|\n",
      "+---------+--------+----------+---------+\n",
      "|        1|Artist_1|Electronic|   France|\n",
      "|        2|Artist_2|Electronic|Australia|\n",
      "|        3|Artist_3|      Jazz|   France|\n",
      "|        4|Artist_4| Classical|Australia|\n",
      "|        5|Artist_5|   Hip-Hop|      USA|\n",
      "+---------+--------+----------+---------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "artists_schema = StructType([\n",
    "    StructField(\"artist_id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"genre\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True)\n",
    "])\n",
    "\n",
    "filepath = str(pathlib.Path().cwd().parent / \"data\" / \"Spotify_Artists.csv\")\n",
    "\n",
    "artists_df = sparksession.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"delimiter\", \",\") \\\n",
    "    .option(\"mode\", \"PERMISSIVE\") \\\n",
    "    .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\") \\\n",
    "    .schema(artists_schema) \\\n",
    "    .csv(filepath)\n",
    "    \n",
    "artists_df.show(5)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5adcae2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 5ï¸âƒ£1ï¸âƒ£ `Window.partitionBy().orderBy()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Defines window specification.\n",
    "\n",
    "### **Why**\n",
    "\n",
    "Required for window functions (ranking, running totals).\n",
    "\n",
    "### **How**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ec4cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b4df5c",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "w = Window.partitionBy(\"dept\").orderBy(\"salary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b3e01f",
   "metadata": {},
   "source": [
    "### **Interview**\n",
    "\n",
    "> Window functions **do not reduce rows**\n",
    "\n",
    "---\n",
    "\n",
    "## 5ï¸âƒ£2ï¸âƒ£ `row_number()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Sequential numbering.\n",
    "\n",
    "### **Use Case**\n",
    "\n",
    "Deduplication, top-N.\n",
    "\n",
    "### **How**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6770460b",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import row_number\n",
    "df.withColumn(\"rn\", row_number().over(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4d35d8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5ï¸âƒ£3ï¸âƒ£ `rank()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Ranking with gaps.\n",
    "\n",
    "### **Interview**\n",
    "\n",
    "> Ties â†’ skipped numbers\n",
    "\n",
    "---\n",
    "\n",
    "## 5ï¸âƒ£4ï¸âƒ£ `dense_rank()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Ranking without gaps.\n",
    "\n",
    "---\n",
    "\n",
    "## 5ï¸âƒ£5ï¸âƒ£ `lead()` / `lag()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Access next/previous row.\n",
    "\n",
    "### **Use Case**\n",
    "\n",
    "Time-series comparisons.\n",
    "\n",
    "### **How**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a9b2eb",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lag\n",
    "df.withColumn(\"prev_salary\", lag(\"salary\").over(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02cdbdc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5ï¸âƒ£6ï¸âƒ£ `sum().over()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Running total.\n",
    "\n",
    "### **How**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab563a07",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "df.withColumn(\"running_sum\", sum(\"salary\").over(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcb1451",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5ï¸âƒ£7ï¸âƒ£ `when()` / `otherwise()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Conditional logic.\n",
    "\n",
    "### **Why**\n",
    "\n",
    "Replaces SQL CASE WHEN.\n",
    "\n",
    "### **How**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92c406f",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "df.withColumn(\"grade\",\n",
    "    when(col(\"salary\") > 80000, \"A\")\n",
    "    .otherwise(\"B\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b860552",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5ï¸âƒ£8ï¸âƒ£ `lit()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Creates literal column.\n",
    "\n",
    "### **How**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc828b9e",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "df.withColumn(\"country\", lit(\"India\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93629859",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5ï¸âƒ£9ï¸âƒ£ `concat()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Concatenates columns.\n",
    "\n",
    "---\n",
    "\n",
    "## 6ï¸âƒ£0ï¸âƒ£ `regexp_replace()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Regex substitution.\n",
    "\n",
    "### **Use Case**\n",
    "\n",
    "Data cleansing.\n",
    "\n",
    "### **How**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96acaa8",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "df.withColumn(\"phone\", regexp_replace(\"phone\", \"-\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5dce15",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6ï¸âƒ£1ï¸âƒ£ `split()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Splits string into array.\n",
    "\n",
    "---\n",
    "\n",
    "## 6ï¸âƒ£2ï¸âƒ£ `explode()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Converts array â†’ rows.\n",
    "\n",
    "### **How**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d8f855",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "df.select(explode(\"skills\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da4f1d1",
   "metadata": {},
   "source": [
    "### **Interview**\n",
    "\n",
    "> `explode()` increases row count\n",
    "\n",
    "---\n",
    "\n",
    "## 6ï¸âƒ£3ï¸âƒ£ `collect_list()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Aggregates into array.\n",
    "\n",
    "---\n",
    "\n",
    "## 6ï¸âƒ£4ï¸âƒ£ `collect_set()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Unique values only.\n",
    "\n",
    "---\n",
    "\n",
    "## 6ï¸âƒ£5ï¸âƒ£ `to_date()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "String â†’ date.\n",
    "\n",
    "### **How**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f3306a",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "df.withColumn(\"dt\", to_date(\"date_str\", \"yyyy-MM-dd\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20f0cc4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6ï¸âƒ£6ï¸âƒ£ `datediff()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Difference between dates.\n",
    "\n",
    "---\n",
    "\n",
    "## 6ï¸âƒ£7ï¸âƒ£ `current_date()` / `current_timestamp()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "System date/time.\n",
    "\n",
    "---\n",
    "\n",
    "## 6ï¸âƒ£8ï¸âƒ£ `year()` / `month()` / `dayofmonth()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Date extraction.\n",
    "\n",
    "---\n",
    "\n",
    "## 6ï¸âƒ£9ï¸âƒ£ `from_json()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "JSON string â†’ struct.\n",
    "\n",
    "### **How**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa2bf2a",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json\n",
    "df.withColumn(\"json_col\", from_json(\"raw_json\", schema))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007c182a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7ï¸âƒ£0ï¸âƒ£ `to_json()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Struct â†’ JSON string.\n",
    "\n",
    "---\n",
    "\n",
    "## 7ï¸âƒ£1ï¸âƒ£ `get_json_object()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Extract JSON field.\n",
    "\n",
    "### **Interview**\n",
    "\n",
    "> Slower than `from_json`\n",
    "\n",
    "---\n",
    "\n",
    "## 7ï¸âƒ£2ï¸âƒ£ `udf()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "User-Defined Function.\n",
    "\n",
    "### **Why**\n",
    "\n",
    "Custom logic.\n",
    "\n",
    "### **How**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dca4cd",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "@udf(IntegerType())\n",
    "def square(x):\n",
    "    return x * x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9de6cfa",
   "metadata": {},
   "source": [
    "### **Interview Warning**\n",
    "\n",
    "âŒ **Avoid UDFs** if built-in exists\n",
    "\n",
    "---\n",
    "\n",
    "## 7ï¸âƒ£3ï¸âƒ£ `pandas_udf()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Vectorized UDF.\n",
    "\n",
    "### **Why**\n",
    "\n",
    "Much faster than normal UDF.\n",
    "\n",
    "---\n",
    "\n",
    "## 7ï¸âƒ£4ï¸âƒ£ `mapPartitions()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Processes partition-wise.\n",
    "\n",
    "### **Use Case**\n",
    "\n",
    "External API calls.\n",
    "\n",
    "---\n",
    "\n",
    "## 7ï¸âƒ£5ï¸âƒ£ `foreachPartition()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Side-effects (write to DB).\n",
    "\n",
    "### **Interview**\n",
    "\n",
    "> Used in **streaming sinks**\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ”¥ Interview Coverage (Part 3)\n",
    "\n",
    "âœ” Window analytics\n",
    "âœ” Complex types\n",
    "âœ” UDF vs built-in performance\n",
    "âœ” Date & JSON handling\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ‘‰ Final Part:\n",
    "\n",
    "**PART 4 (76â€“100):**\n",
    "\n",
    "* Writing data\n",
    "* File formats\n",
    "* Partitioning strategies\n",
    "* Spark internals\n",
    "* Performance tuning\n",
    "* Streaming basics\n",
    "\n",
    "Say **â€œContinue Part 4â€**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
