{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1564d885",
   "metadata": {},
   "source": [
    "Below is a **deep, internal, PySpark-specific explanation of *every* Spark architecture component**, from Python API â†’ JVM â†’ cluster execution. This is written from a **data-engineering + interview + production debugging** perspective.\n",
    "\n",
    "---\n",
    "\n",
    "## PySpark Architecture â€“ Internal Working (End-to-End)\n",
    "\n",
    "![Image](https://substackcdn.com/image/fetch/%24s_%21RGKt%21%2Cf_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F383f19cc-ec30-49cd-a99f-a2b72a2bed34_1626x1232.png)\n",
    "\n",
    "![Image](https://miro.medium.com/v2/resize%3Afit%3A596/1%2Ax3wPOV8MIzquLSgEKXlUag.png)\n",
    "\n",
    "![Image](https://books.japila.pl/apache-spark-internals/images/scheduler/dagscheduler-rdd-lineage-stage-dag.png)\n",
    "\n",
    "![Image](https://books.japila.pl/apache-spark-internals/images/scheduler/dagscheduler-new-instance.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 0. Big Picture (What PySpark *Really* Is)\n",
    "\n",
    "**Apache Spark** is a **JVM-based execution engine**.\n",
    "\n",
    "ðŸ‘‰ **PySpark is NOT Spark**\n",
    "ðŸ‘‰ PySpark is a **Python wrapper** that talks to Spark **running on JVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e88694",
   "metadata": {},
   "outputs": [],
   "source": [
    "Python Code\n",
    "  â†“\n",
    "PySpark API\n",
    "  â†“ (Py4J)\n",
    "Spark JVM (Driver)\n",
    "  â†“\n",
    "Executors (JVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083574aa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1. PySpark Entry Layer (Python Side)\n",
    "\n",
    "### 1.1 PySpark API (Python Process)\n",
    "\n",
    "When you write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528de808",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2e5a9a",
   "metadata": {},
   "source": [
    "Internally:\n",
    "\n",
    "* A **Python process** starts\n",
    "* PySpark creates:\n",
    "\n",
    "  * `SparkSession`\n",
    "  * `SparkContext`\n",
    "* These are **thin Python proxies**\n",
    "\n",
    "ðŸ“Œ **No computation happens in Python**\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2 Py4J Bridge (CRITICAL INTERNAL)\n",
    "\n",
    "PySpark uses **Py4J** to communicate with JVM.\n",
    "\n",
    "What Py4J does:\n",
    "\n",
    "* Serializes Python calls\n",
    "* Sends them to JVM via sockets\n",
    "* Receives JVM objects back as **JavaObject proxies**\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c6656e",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "df.filter(df.age > 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eed62c0",
   "metadata": {},
   "source": [
    "Actually becomes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70efbc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "Python â†’ Py4J â†’ JVM Logical Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ffb478",
   "metadata": {},
   "source": [
    "ðŸ“Œ Heavy Python logic = BAD\n",
    "ðŸ“Œ Spark execution = JVM only\n",
    "\n",
    "---\n",
    "\n",
    "# 2. Spark Driver Internals (JVM Side)\n",
    "\n",
    "The **Driver** is the *brain* of Spark.\n",
    "\n",
    "---\n",
    "\n",
    "## 2.1 SparkContext (Low-Level Brain)\n",
    "\n",
    "Created automatically by SparkSession.\n",
    "\n",
    "Responsibilities:\n",
    "\n",
    "* Application lifecycle\n",
    "* Cluster communication\n",
    "* RDD creation\n",
    "* Job submission\n",
    "\n",
    "Internal objects:\n",
    "\n",
    "* `DAGScheduler`\n",
    "* `TaskScheduler`\n",
    "* `SchedulerBackend`\n",
    "* `BlockManagerMaster`\n",
    "\n",
    "---\n",
    "\n",
    "## 2.2 SparkSession (High-Level Entry)\n",
    "\n",
    "SparkSession internally wraps:\n",
    "\n",
    "* SparkContext\n",
    "* SQLContext\n",
    "* HiveContext (optional)\n",
    "\n",
    "Used for:\n",
    "\n",
    "* DataFrame / SQL API\n",
    "* Catalyst Optimizer\n",
    "* Tungsten Engine\n",
    "\n",
    "---\n",
    "\n",
    "# 3. Logical Planning (Catalyst Optimizer)\n",
    "\n",
    "### 3.1 Logical Plan Creation\n",
    "\n",
    "Your PySpark code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8ba871",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "df.filter(\"age > 30\").groupBy(\"dept\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cbc3d4",
   "metadata": {},
   "source": [
    "Turns into:\n",
    "\n",
    "* **Unresolved Logical Plan**\n",
    "* Then **Resolved Logical Plan**\n",
    "\n",
    "Operations:\n",
    "\n",
    "* Column resolution\n",
    "* Type inference\n",
    "* Function binding\n",
    "\n",
    "---\n",
    "\n",
    "### 3.2 Catalyst Optimizer (RULE ENGINE)\n",
    "\n",
    "Catalyst applies **rule-based optimization**:\n",
    "\n",
    "Examples:\n",
    "\n",
    "* Predicate pushdown\n",
    "* Column pruning\n",
    "* Constant folding\n",
    "* Filter reordering\n",
    "\n",
    "ðŸ“Œ This happens **before any execution**\n",
    "\n",
    "---\n",
    "\n",
    "### 3.3 Physical Plan Generation\n",
    "\n",
    "Logical plan â†’ multiple physical plans â†’ best chosen\n",
    "\n",
    "Physical operators:\n",
    "\n",
    "* `HashAggregateExec`\n",
    "* `SortMergeJoinExec`\n",
    "* `BroadcastHashJoinExec`\n",
    "\n",
    "ðŸ“Œ This determines **performance**\n",
    "\n",
    "---\n",
    "\n",
    "# 4. DAG Scheduler (Execution Planner)\n",
    "\n",
    "## 4.1 DAG Creation\n",
    "\n",
    "Spark builds a **Directed Acyclic Graph** of transformations.\n",
    "\n",
    "Transformation types:\n",
    "\n",
    "* **Narrow** (map, filter)\n",
    "* **Wide** (groupBy, join)\n",
    "\n",
    "Wide transformations create **shuffle boundaries**\n",
    "\n",
    "---\n",
    "\n",
    "## 4.2 Job â†’ Stage â†’ Task Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cfb3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Action\n",
    "  â†“\n",
    "Job\n",
    "  â†“\n",
    "Stages (shuffle boundaries)\n",
    "  â†“\n",
    "Tasks (1 per partition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8648a67f",
   "metadata": {},
   "source": [
    "ðŸ“Œ Tasks are the **smallest execution unit**\n",
    "\n",
    "---\n",
    "\n",
    "## 4.3 Fault Tolerance via Lineage\n",
    "\n",
    "RDD/DataFrame lineage:\n",
    "\n",
    "* Keeps transformation history\n",
    "* If partition fails â†’ recompute\n",
    "\n",
    "ðŸ“Œ No checkpoint unless explicitly asked\n",
    "\n",
    "---\n",
    "\n",
    "# 5. Task Scheduler (Low-Level Execution)\n",
    "\n",
    "### Responsibilities:\n",
    "\n",
    "* Assign tasks to executors\n",
    "* Handle retries\n",
    "* Enforce locality:\n",
    "\n",
    "  * PROCESS_LOCAL\n",
    "  * NODE_LOCAL\n",
    "  * RACK_LOCAL\n",
    "  * ANY\n",
    "\n",
    "ðŸ“Œ Spark prefers **data locality**\n",
    "\n",
    "---\n",
    "\n",
    "# 6. Cluster Manager Interaction\n",
    "\n",
    "Spark requests resources from cluster manager.\n",
    "\n",
    "Supported:\n",
    "\n",
    "* **YARN**\n",
    "* **Kubernetes**\n",
    "* Standalone\n",
    "\n",
    "Cluster manager:\n",
    "\n",
    "* Allocates CPU + memory\n",
    "* Launches executors\n",
    "\n",
    "ðŸ“Œ Cluster manager does **NOT** execute tasks\n",
    "\n",
    "---\n",
    "\n",
    "# 7. Executor Internals (Most Important)\n",
    "\n",
    "Executors are **long-running JVM processes**.\n",
    "\n",
    "---\n",
    "\n",
    "## 7.1 Executor Components\n",
    "\n",
    "Each executor contains:\n",
    "\n",
    "* Task threads\n",
    "* BlockManager\n",
    "* ShuffleManager\n",
    "* MemoryManager\n",
    "* JVM heap\n",
    "\n",
    "---\n",
    "\n",
    "## 7.2 Executor Memory Model (Unified Memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b8d1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Executor Heap\n",
    " â”œâ”€â”€ Reserved Memory (~300MB)\n",
    " â”œâ”€â”€ Execution Memory\n",
    " â”‚     â””â”€â”€ joins, sorts, shuffles\n",
    " â”œâ”€â”€ Storage Memory\n",
    " â”‚     â””â”€â”€ cache/persist\n",
    " â””â”€â”€ User Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cde0559",
   "metadata": {},
   "source": [
    "Dynamic sharing between execution & storage.\n",
    "\n",
    "ðŸ“Œ Memory pressure â†’ spills to disk\n",
    "\n",
    "---\n",
    "\n",
    "## 7.3 BlockManager (Data Storage)\n",
    "\n",
    "Stores:\n",
    "\n",
    "* Cached DataFrames\n",
    "* Shuffle files\n",
    "* Broadcast variables\n",
    "\n",
    "Block types:\n",
    "\n",
    "* MEMORY_ONLY\n",
    "* MEMORY_AND_DISK\n",
    "* DISK_ONLY\n",
    "\n",
    "---\n",
    "\n",
    "# 8. Shuffle Internals (Performance Killer)\n",
    "\n",
    "Shuffle happens when:\n",
    "\n",
    "* groupBy\n",
    "* join\n",
    "* distinct\n",
    "* repartition\n",
    "\n",
    "---\n",
    "\n",
    "### 8.1 Shuffle Write Phase\n",
    "\n",
    "* Map tasks write shuffle files\n",
    "* Partitioned by hash / range\n",
    "\n",
    "### 8.2 Shuffle Read Phase\n",
    "\n",
    "* Reduce tasks fetch blocks\n",
    "* Network transfer\n",
    "* Merge & sort\n",
    "\n",
    "ðŸ“Œ Shuffles:\n",
    "\n",
    "* Cause disk IO\n",
    "* Cause network IO\n",
    "* Cause GC pressure\n",
    "\n",
    "---\n",
    "\n",
    "# 9. Tungsten Engine (Low-Level Optimization)\n",
    "\n",
    "Tungsten provides:\n",
    "\n",
    "* Off-heap memory\n",
    "* Binary row format\n",
    "* Cache-friendly layout\n",
    "* Whole-stage codegen\n",
    "\n",
    "---\n",
    "\n",
    "### 9.1 Whole-Stage Code Generation\n",
    "\n",
    "Multiple operators compiled into **single Java function**.\n",
    "\n",
    "Benefits:\n",
    "\n",
    "* Fewer virtual calls\n",
    "* CPU cache efficiency\n",
    "* SIMD-like behavior\n",
    "\n",
    "---\n",
    "\n",
    "# 10. Python UDF Execution (Special Case)\n",
    "\n",
    "### Normal operations:\n",
    "\n",
    "* JVM only (fast)\n",
    "\n",
    "### Python UDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b488d342",
   "metadata": {},
   "outputs": [],
   "source": [
    "Executor JVM\n",
    "  â†”\n",
    "Python Worker Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d421cc8",
   "metadata": {},
   "source": [
    "Costs:\n",
    "\n",
    "* Serialization\n",
    "* Context switching\n",
    "\n",
    "ðŸ“Œ Prefer:\n",
    "\n",
    "* Built-in functions\n",
    "* Pandas UDFs (Arrow)\n",
    "\n",
    "---\n",
    "\n",
    "# 11. Arrow Optimization (PySpark â†” Pandas)\n",
    "\n",
    "Arrow:\n",
    "\n",
    "* Columnar in-memory format\n",
    "* Zero-copy transfer\n",
    "\n",
    "Used in:\n",
    "\n",
    "* Pandas UDF\n",
    "* toPandas()\n",
    "\n",
    "ðŸ“Œ Reduces serialization overhead\n",
    "\n",
    "---\n",
    "\n",
    "# 12. Fault Tolerance (End-to-End)\n",
    "\n",
    "Failure handling:\n",
    "\n",
    "* Task retry\n",
    "* Executor restart\n",
    "* Node loss recovery\n",
    "* Speculative execution\n",
    "\n",
    "ðŸ“Œ Spark is **eventually consistent**\n",
    "\n",
    "---\n",
    "\n",
    "# 13. End-to-End Flow (One Shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9087f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Python Code\n",
    " â†“\n",
    "PySpark API\n",
    " â†“ (Py4J)\n",
    "Logical Plan\n",
    " â†“\n",
    "Catalyst Optimizer\n",
    " â†“\n",
    "Physical Plan\n",
    " â†“\n",
    "DAG Scheduler\n",
    " â†“\n",
    "Stages â†’ Tasks\n",
    " â†“\n",
    "Task Scheduler\n",
    " â†“\n",
    "Executors\n",
    " â†“\n",
    "Shuffle / Compute\n",
    " â†“\n",
    "Result to Driver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de8704d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 14. Interview-Grade Summary\n",
    "\n",
    "> **PySpark is a Python front-end over a JVM-based distributed execution engine where all optimization, scheduling, memory management, and execution occur inside Sparkâ€™s Driver and Executors using DAG-based execution, Catalyst optimization, Tungsten engine, and unified memory management.**\n",
    "\n",
    "---\n",
    "\n",
    "## What I can do next (recommended for you ðŸ‘‡)\n",
    "\n",
    "Given your **data engineering focus**, next best deep dives:\n",
    "\n",
    "1. **Spark DAG vs Physical Plan vs Execution Plan (with examples)**\n",
    "2. **Executor memory tuning (spark.memory.*)**\n",
    "3. **Shuffle tuning strategies**\n",
    "4. **PySpark performance anti-patterns**\n",
    "5. **Spark on Kubernetes â€“ internal pod architecture**\n",
    "6. **How Spark uses CPU cache, SIMD & JVM JIT**\n",
    "\n",
    "Tell me which one you want â€” Iâ€™ll go *even deeper*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
