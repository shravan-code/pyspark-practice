## Window Functions in PySpark

Apache Spark window functions let you perform **analytics across related rows** *without collapsing rows* (unlike `groupBy`). They’re essential for rankings, running totals, moving averages, and session-style analysis.

---

### What a Window Function Does

A window function computes a value for **each row** based on:

* **Partition** → how rows are grouped
* **Order** → how rows are sorted within each group
* **Frame** → which rows around the current row are considered

![Image](https://miro.medium.com/1%2A74UtrCWGqZZXE4gS4m6P8A.gif)

![Image](https://knockdata.github.io/images/spark-window-function-basic-window.png)

---

## 1) Define a Window Specification

```python
from pyspark.sql.window import Window

window_spec = (
    Window
    .partitionBy("department")
    .orderBy("salary")
)
```

Optionally add a **frame**:

```python
window_spec = (
    Window
    .partitionBy("department")
    .orderBy("date")
    .rowsBetween(Window.unboundedPreceding, Window.currentRow)
)
```

---

## 2) Common Window Functions

### Ranking Functions

```python
from pyspark.sql.functions import row_number, rank, dense_rank

df.withColumn("row_num", row_number().over(window_spec))
df.withColumn("rank", rank().over(window_spec))
df.withColumn("dense_rank", dense_rank().over(window_spec))
```

**Differences**

* `row_number()` → unique sequence (no ties)
* `rank()` → gaps after ties
* `dense_rank()` → no gaps

![Image](https://miro.medium.com/1%2AtuGFvhwk5rUtoQWcX4A6ng.gif)

![Image](https://media.licdn.com/dms/image/v2/D4D12AQFRS0AU_T_NQQ/article-cover_image-shrink_720_1280/article-cover_image-shrink_720_1280/0/1654768699532?e=2147483647\&t=ONuIV9v8k99yhD5U52ocS8i3WOsKk_p_xlfjmGVVXEg\&v=beta)

---

### Aggregate Functions (Windowed)

```python
from pyspark.sql.functions import sum, avg, max

df.withColumn("dept_avg_salary", avg("salary").over(window_spec))
df.withColumn("running_total", sum("sales").over(window_spec))
```

> Same aggregates as `groupBy`, but **row-level output is preserved**.

---

### Analytical / Value Functions

```python
from pyspark.sql.functions import lag, lead, first, last

df.withColumn("prev_salary", lag("salary", 1).over(window_spec))
df.withColumn("next_salary", lead("salary", 1).over(window_spec))
df.withColumn("first_sal", first("salary").over(window_spec))
df.withColumn("last_sal", last("salary").over(window_spec))
```

Use `ignoreNulls=True` when needed:

```python
last("salary", ignorenulls=True).over(window_spec)
```

---

## 3) Rows vs Range Frames (Very Important)

### Rows-based frame

```python
Window.orderBy("date").rowsBetween(-2, 0)
```

* Counts **physical rows**
* Deterministic with strict ordering

### Range-based frame

```python
Window.orderBy("date").rangeBetween(-7, 0)
```

* Uses **value ranges**
* Multiple rows can share the same order key

> For time series with duplicate timestamps, **rowsBetween** is safer.

---

## 4) Practical Data Engineering Examples

### Latest Record per Key (De-duplication)

```python
w = Window.partitionBy("user_id").orderBy(col("updated_at").desc())

df.filter(row_number().over(w) == 1)
```

### Top-N per Group

```python
w = Window.partitionBy("category").orderBy(col("revenue").desc())

df.withColumn("r", dense_rank().over(w)) \
  .filter("r <= 3")
```

### Running Metrics (Finance / Logs)

```python
w = Window.partitionBy("account").orderBy("date") \
          .rowsBetween(Window.unboundedPreceding, Window.currentRow)

df.withColumn("balance", sum("amount").over(w))
```

---

## 5) Performance Notes (Critical for Interviews)

* Window functions **cause shuffles** if `partitionBy` changes data distribution
* Prefer **low-cardinality partitions**
* Avoid very wide frames on huge datasets
* Cache if the same window spec is reused
* Window ≠ groupBy: window keeps **N rows**, groupBy reduces to **K rows**

---

## Quick Mental Model

| Aspect         | groupBy | Window |
| -------------- | ------- | ------ |
| Rows preserved | ❌       | ✅      |
| Aggregation    | ✅       | ✅      |
| Ranking / Lag  | ❌       | ✅      |
| Shuffle        | Yes     | Yes    |

---

If you want, I can next:

* Deep-dive into **window execution internals (DAG + shuffle)**
* Compare **SQL window functions vs PySpark**
* Give **interview-grade window function questions**
* Optimize a **real dataset window query** you’re working on
