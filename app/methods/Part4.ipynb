{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e303b447",
   "metadata": {},
   "source": [
    "Final stretch. This part focuses on **production readiness + performance + internals** ‚Äî exactly what **senior interviews and real projects** test.\n",
    "\n",
    "---\n",
    "\n",
    "# ‚úÖ PySpark Top 100 Methods ‚Äî **PART 4 (76‚Äì100)**\n",
    "\n",
    "**Category: Writing Data, File Formats, Partitioning, Performance, Internals, Streaming**\n",
    "\n",
    "---\n",
    "\n",
    "## 7Ô∏è‚É£6Ô∏è‚É£ `df.write`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Entry point for writing data.\n",
    "\n",
    "### **Why**\n",
    "\n",
    "Persists DataFrame to storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2043bcb7",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "df.write.parquet(\"output/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6a38ab",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7Ô∏è‚É£7Ô∏è‚É£ `df.write.format()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Explicit output format.\n",
    "\n",
    "### **Why**\n",
    "\n",
    "Production pipelines require explicit formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54ddbb3",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "df.write.format(\"parquet\").save(\"s3://bucket/path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b0f327",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7Ô∏è‚É£8Ô∏è‚É£ `mode()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Write mode.\n",
    "\n",
    "| Mode      | Meaning |\n",
    "| --------- | ------- |\n",
    "| overwrite | Replace |\n",
    "| append    | Add     |\n",
    "| ignore    | Skip    |\n",
    "| error     | Fail    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61f70ca",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").parquet(\"out/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933fe64b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7Ô∏è‚É£9Ô∏è‚É£ `partitionBy()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Creates directory-based partitions.\n",
    "\n",
    "### **Why**\n",
    "\n",
    "Enables **partition pruning**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09d0af1",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "df.write.partitionBy(\"year\", \"month\").parquet(\"out/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6189e73",
   "metadata": {},
   "source": [
    "### **Interview**\n",
    "\n",
    "> Too many partitions = small file problem\n",
    "\n",
    "---\n",
    "\n",
    "## 8Ô∏è‚É£0Ô∏è‚É£ `bucketBy()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Bucketing data.\n",
    "\n",
    "### **Why**\n",
    "\n",
    "Optimizes joins & aggregations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d0f9db",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "df.write.bucketBy(10, \"user_id\").sortBy(\"user_id\").saveAsTable(\"users\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1325637d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8Ô∏è‚É£1Ô∏è‚É£ `saveAsTable()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Writes to Hive metastore.\n",
    "\n",
    "### **Use Case**\n",
    "\n",
    "Spark SQL + BI tools.\n",
    "\n",
    "---\n",
    "\n",
    "## 8Ô∏è‚É£2Ô∏è‚É£ `insertInto()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Inserts into existing table.\n",
    "\n",
    "### **Interview**\n",
    "\n",
    "> Schema must match exactly\n",
    "\n",
    "---\n",
    "\n",
    "## 8Ô∏è‚É£3Ô∏è‚É£ `parquet()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Columnar storage.\n",
    "\n",
    "### **Why**\n",
    "\n",
    "Best default format for Spark.\n",
    "\n",
    "### **Interview**\n",
    "\n",
    "> Supports predicate pushdown + compression\n",
    "\n",
    "---\n",
    "\n",
    "## 8Ô∏è‚É£4Ô∏è‚É£ `orc()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Optimized for Hive.\n",
    "\n",
    "---\n",
    "\n",
    "## 8Ô∏è‚É£5Ô∏è‚É£ `json()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Semi-structured output.\n",
    "\n",
    "### **Downside**\n",
    "\n",
    "‚ùå No schema enforcement\n",
    "\n",
    "---\n",
    "\n",
    "## 8Ô∏è‚É£6Ô∏è‚É£ `csv()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Text-based format.\n",
    "\n",
    "### **Interview**\n",
    "\n",
    "> Avoid for large-scale analytics\n",
    "\n",
    "---\n",
    "\n",
    "## 8Ô∏è‚É£7Ô∏è‚É£ `option(\"compression\")`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Compression type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674b68cd",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "df.write.option(\"compression\", \"snappy\").parquet(\"out/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fe2c54",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8Ô∏è‚É£8Ô∏è‚É£ `checkpoint()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Truncates lineage.\n",
    "\n",
    "### **Why**\n",
    "\n",
    "Prevents long DAG failures.\n",
    "\n",
    "---\n",
    "\n",
    "## 8Ô∏è‚É£9Ô∏è‚É£ `unpersist()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Removes cached data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef425f1",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "df.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f50c3a9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9Ô∏è‚É£0Ô∏è‚É£ `spark.sql()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Run SQL queries.\n",
    "\n",
    "### **Why**\n",
    "\n",
    "Easy migration from RDBMS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830c7865",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3191be",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9Ô∏è‚É£1Ô∏è‚É£ `createOrReplaceTempView()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Temporary SQL view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a786c25d",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"emp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b277f8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9Ô∏è‚É£2Ô∏è‚É£ `explain(\"formatted\")`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Detailed execution plan.\n",
    "\n",
    "### **Interview**\n",
    "\n",
    "> Understand **Scan ‚Üí Filter ‚Üí Exchange ‚Üí Aggregate**\n",
    "\n",
    "---\n",
    "\n",
    "## 9Ô∏è‚É£3Ô∏è‚É£ `spark.conf.set()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Set Spark configs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b603e1c5",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f22df34",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9Ô∏è‚É£4Ô∏è‚É£ `spark.sql.shuffle.partitions`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Default shuffle partitions (200).\n",
    "\n",
    "### **Interview**\n",
    "\n",
    "> Tune this aggressively\n",
    "\n",
    "---\n",
    "\n",
    "## 9Ô∏è‚É£5Ô∏è‚É£ `spark.sql.adaptive.enabled`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Adaptive Query Execution (AQE).\n",
    "\n",
    "### **Why**\n",
    "\n",
    "Dynamic optimization.\n",
    "\n",
    "---\n",
    "\n",
    "## 9Ô∏è‚É£6Ô∏è‚É£ `spark.readStream`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Streaming source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af541e4",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "df = spark.readStream.format(\"kafka\").load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60aa26d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9Ô∏è‚É£7Ô∏è‚É£ `writeStream`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Streaming sink."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ad11d2",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "df.writeStream.format(\"console\").start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ee2d50",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9Ô∏è‚É£8Ô∏è‚É£ `trigger()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Controls micro-batch timing.\n",
    "\n",
    "---\n",
    "\n",
    "## 9Ô∏è‚É£9Ô∏è‚É£ `foreachBatch()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Custom batch processing.\n",
    "\n",
    "### **Use Case**\n",
    "\n",
    "Streaming ‚Üí DB writes.\n",
    "\n",
    "---\n",
    "\n",
    "## üîü0Ô∏è‚É£ `awaitTermination()`\n",
    "\n",
    "### **What**\n",
    "\n",
    "Keeps streaming alive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6abc6e0",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f878bb8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üéØ FULL INTERVIEW READINESS ACHIEVED\n",
    "\n",
    "You now have:\n",
    "\n",
    "* **Top 100 PySpark methods**\n",
    "* **What / Why / How / Scenario**\n",
    "* **Performance + internals**\n",
    "* **Production & streaming**\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Next-Level Add-Ons (Highly Recommended)\n",
    "\n",
    "I can now help you with:\n",
    "\n",
    "1. **PySpark vs SQL mapping (method ‚Üí SQL)**\n",
    "2. **50 real interview scenarios with answers**\n",
    "3. **Spark DAG & Catalyst optimizer deep dive**\n",
    "4. **Hands-on mini projects per category**\n",
    "5. **PySpark performance tuning checklist**\n",
    "\n",
    "Just tell me **what you want next**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
