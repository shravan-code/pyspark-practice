{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a3c466f",
   "metadata": {},
   "source": "## Apache Spark Architecture (Conceptual + Execution View)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "![Image](https://spark.apache.org/docs/latest/img/cluster-overview.png)\n",
    "\n",
    "![Image](https://docs.aws.amazon.com/images/prescriptive-guidance/latest/tuning-aws-glue-for-apache-spark/images/architecture-driver-cluster-worker.png)"
   ],
   "id": "dd7279c9daa6fc38"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## 1. High-Level Architecture\n",
    "\n",
    "**Apache Spark** follows a **masterâ€“worker** architecture optimized for in-memory, distributed computation."
   ],
   "id": "302b9dff817f341f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* User Code\n",
    "*     Driver Program\n",
    "*         Cluster Manager\n",
    "*             Executors (on Worker Nodes)"
   ],
   "id": "97d71f05aebeeb0d"
  },
  {
   "cell_type": "markdown",
   "id": "4fc4d2a7",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Core Components"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "### 2.1 Driver Program (Brain of Spark)\n",
    "\n",
    "The **Driver** runs your main application and is responsible for:\n",
    "\n",
    "* Creating **SparkSession / SparkContext**\n",
    "* Converting user code â†’ **Logical Plan**\n",
    "* Building **DAG (Directed Acyclic Graph)**\n",
    "* Scheduling jobs, stages, and tasks\n",
    "* Collecting results back to the client\n",
    "\n",
    "ðŸ“Œ **Only ONE driver per Spark application**\n"
   ],
   "id": "a67a65442098f7b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "### 2.2 Cluster Manager (Resource Manager)\n",
    "\n",
    "The **Cluster Manager** allocates resources (CPU, memory) to Spark applications.\n",
    "\n",
    "Supported managers:\n",
    "\n",
    "* **YARN**\n",
    "* **Kubernetes**\n",
    "* **Apache Mesos**\n",
    "* **Standalone Spark**\n",
    "\n",
    "ðŸ“Œ Spark is **cluster-manager agnostic**\n"
   ],
   "id": "933e224aa69e402c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "### 2.3 Worker Nodes\n",
    "\n",
    "Worker nodes are machines that host **Executors**.\n",
    "\n",
    "* Each worker can run **multiple executors**\n",
    "* Workers do NOT run user logic directly\n"
   ],
   "id": "ba0833d27ee6394d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "### 2.4 Executors (Actual Workhorses)\n",
    "\n",
    "Executors are JVM processes launched on worker nodes.\n",
    "\n",
    "Responsibilities:\n",
    "\n",
    "* Execute **tasks**\n",
    "* Store data in **memory / disk**\n",
    "* Cache & persist RDDs / DataFrames\n",
    "* Return results to the Driver\n",
    "\n",
    "ðŸ“Œ Executors live **for the lifetime of the application**\n"
   ],
   "id": "d492c16edfd4af4e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "## 3. Spark Execution Flow (Step-by-Step)\n",
    "\n",
    "### Step 1: Application Submission"
   ],
   "id": "8989819fba0926b5"
  },
  {
   "cell_type": "code",
   "id": "e6e569d0",
   "metadata": {
    "language": "bash",
    "ExecuteTime": {
     "end_time": "2026-02-02T12:48:36.198578700Z",
     "start_time": "2026-02-02T12:48:36.139116800Z"
    }
   },
   "source": "# spark-submit app.py",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "0680ddca",
   "metadata": {},
   "source": [
    "* Driver starts\n",
    "* Requests resources from Cluster Manager\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "### Step 2: Logical Plan Creation\n",
    "\n",
    "User code:"
   ],
   "id": "878722394989d8c6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4facf6d8",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": "# df.filter(...).groupBy(...).agg(...)"
  },
  {
   "cell_type": "markdown",
   "id": "7763520d",
   "metadata": {},
   "source": [
    "Converted into:\n",
    "\n",
    "* **Logical Plan** (what to do)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Step 3: DAG Creation\n",
    "\n",
    "Spark builds a **DAG** of transformations:\n",
    "\n",
    "* **Narrow transformations** â†’ same stage\n",
    "* **Wide transformations** â†’ shuffle â†’ new stage\n",
    "\n",
    "ðŸ“Œ Wide transformations trigger **shuffle**\n"
   ],
   "id": "b9cb08133f60fd2d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "### Step 4: Stages & Tasks\n",
    "\n",
    "* DAG â†’ **Stages**\n",
    "* Stages â†’ **Tasks**\n",
    "* One task = one partition"
   ],
   "id": "a86e95a322757e6"
  },
  {
   "cell_type": "code",
   "id": "0f3a58a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T12:50:37.953993900Z",
     "start_time": "2026-02-02T12:50:37.841123400Z"
    }
   },
   "source": [
    "# Job\n",
    "#  â”œâ”€â”€ Stage 1 (map)\n",
    "#  â”‚    â”œâ”€â”€ Task 1\n",
    "#  â”‚    â”œâ”€â”€ Task 2\n",
    "#  â””â”€â”€ Stage 2 (shuffle)\n",
    "#       â”œâ”€â”€ Task 3\n",
    "#       â”œâ”€â”€ Task 4"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "8e16d4a4",
   "metadata": {},
   "source": [
    "\n",
    "### Step 5: Task Execution\n",
    "\n",
    "* Driver sends tasks to Executors\n",
    "* Executors process partitions in parallel\n",
    "* Results sent back to Driver\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "## 4. Spark Internal Schedulers\n",
    "\n",
    "### 4.1 DAG Scheduler\n",
    "\n",
    "* Splits jobs into stages\n",
    "* Handles **fault recovery**\n",
    "* Optimizes execution graph\n",
    "\n",
    "### 4.2 Task Scheduler\n",
    "\n",
    "* Sends tasks to executors\n",
    "* Handles retries\n",
    "* Ensures locality (data locality awareness)\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Spark Memory Architecture (Executor Level)\n",
    "\n",
    "Each executor memory is divided into:"
   ],
   "id": "4df855cd94883e1f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466266f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Executor Memory\n",
    " â”œâ”€â”€ Execution Memory (shuffle, join, sort)\n",
    " â”œâ”€â”€ Storage Memory (cache/persist)\n",
    " â”œâ”€â”€ User Memory\n",
    " â””â”€â”€ Reserved Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006b1103",
   "metadata": {},
   "source": [
    "ðŸ“Œ Spark uses **unified memory management**\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Spark Data Abstractions in Architecture\n",
    "\n",
    "| Layer      | Abstraction          |\n",
    "| ---------- | -------------------- |\n",
    "| Low-level  | RDD                  |\n",
    "| Structured | DataFrame            |\n",
    "| SQL        | Spark SQL            |\n",
    "| Streaming  | Structured Streaming |\n",
    "\n",
    "All compile down to **RDD-based execution**\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Fault Tolerance (Key Architectural Strength)\n",
    "\n",
    "Spark achieves fault tolerance using:\n",
    "\n",
    "* **RDD Lineage**\n",
    "* Task re-execution\n",
    "* Executor restart\n",
    "* Speculative execution (slow tasks)\n",
    "\n",
    "ðŸ“Œ No data replication like HDFS required\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Where Performance Comes From\n",
    "\n",
    "* In-memory processing\n",
    "* Lazy evaluation\n",
    "* DAG optimization\n",
    "* Whole-stage code generation\n",
    "* Tungsten execution engine\n",
    "* Predicate pushdown\n",
    "* Vectorized execution\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Architecture Summary Table\n",
    "\n",
    "| Component       | Role                   |\n",
    "| --------------- | ---------------------- |\n",
    "| Driver          | Orchestrates execution |\n",
    "| Cluster Manager | Allocates resources    |\n",
    "| Worker          | Hosts executors        |\n",
    "| Executor        | Executes tasks         |\n",
    "| DAG Scheduler   | Builds execution plan  |\n",
    "| Task Scheduler  | Runs tasks             |\n",
    "| Shuffle Service | Data exchange          |\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Interview-Ready One-Liner\n",
    "\n",
    "> **Spark architecture is a driver-driven, DAG-based, distributed execution engine that uses executors on worker nodes to process partitioned data in parallel with in-memory optimization and fault tolerance via lineage.**\n",
    "\n",
    "---\n",
    "\n",
    "If you want, next I can:\n",
    "\n",
    "* Deep dive **DAG vs Stage vs Task**\n",
    "* Explain **Spark on Kubernetes architecture**\n",
    "* Map **Spark architecture to PySpark code**\n",
    "* Compare **Spark vs Hadoop MapReduce architecture**\n",
    "\n",
    "Just tell me which one."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
