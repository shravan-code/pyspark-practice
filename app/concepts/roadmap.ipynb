{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9624f193",
   "metadata": {},
   "source": "# Roadmap for **PySpark**"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "## Step 0: Prerequisites (Mandatory Foundation)\n",
    "\n",
    "Before touching advanced PySpark, you must be **solid** in:\n",
    "\n",
    "### Core Skills\n",
    "\n",
    "* **Python**: iterators, generators, decorators, multiprocessing\n",
    "* **SQL**: joins, window functions, subqueries, query optimization\n",
    "* **Linux**: filesystems, permissions, grep, awk, cron\n",
    "* **Data formats**: CSV, JSON, Parquet, Avro\n",
    "\n",
    "> If SQL & Python arenâ€™t second nature, PySpark performance tuning will fail.\n"
   ],
   "id": "75ad63c07c8d0fd3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "## Step 1: Spark Fundamentals (Mental Model First)\n",
    "\n",
    "Learn **how Spark works internally**, not just syntax.\n",
    "\n",
    "### Concepts to Master\n",
    "\n",
    "* Driver vs Executors\n",
    "* Cluster manager (Standalone / YARN / Kubernetes)\n",
    "* Lazy evaluation\n",
    "* DAG (Directed Acyclic Graph)\n",
    "* Jobs â†’ Stages â†’ Tasks\n",
    "* Narrow vs Wide transformations\n",
    "\n",
    "![Image](https://spark.apache.org/docs/latest/img/cluster-overview.png)\n",
    "\n",
    "\n",
    "### Outcome\n",
    "\n",
    "You should be able to answer:\n",
    "\n",
    "> â€œWhy did my job create 200 tasks when I ran one action?â€\n"
   ],
   "id": "708d583888b429fb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "## Step 2: PySpark Core APIs (Hands-on Daily)\n",
    "\n",
    "Now learn **syntax + behavior**.\n",
    "\n",
    "### DataFrame Operations (Critical)"
   ],
   "id": "7f64d6191e1d2250"
  },
  {
   "cell_type": "code",
   "id": "f95c574e",
   "metadata": {
    "language": "python",
    "ExecuteTime": {
     "end_time": "2026-02-02T12:15:33.477945800Z",
     "start_time": "2026-02-02T12:15:33.364778800Z"
    }
   },
   "source": [
    "# df.select(\"col1\", F.col(\"col2\").alias(\"x\"))\n",
    "# df.filter(F.col(\"age\") > 25)\n",
    "# df.groupBy(\"dept\").agg(F.avg(\"salary\"))\n",
    "# df.withColumn(\"flag\", F.when(F.col(\"x\") > 0, 1).otherwise(0))"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "4a33c6c5",
   "metadata": {},
   "source": [
    "### Must-Know Areas\n",
    "\n",
    "* `select`, `filter`, `withColumn`\n",
    "* `groupBy`, `agg`\n",
    "* `join` (inner, left, broadcast)\n",
    "* `window functions`\n",
    "* `explode`, `collect_list`\n",
    "* `cache vs persist`\n",
    "\n",
    "### Avoid\n",
    "\n",
    "* `.collect()` on large data\n",
    "* Python UDFs (unless unavoidable)\n",
    "\n",
    "---\n",
    "\n",
    "## Step 3: Execution Engine Deep Dive (THIS MAKES YOU PRO)\n",
    "\n",
    "Most PySpark users fail here.\n",
    "\n",
    "### Internals You MUST Know\n",
    "\n",
    "* Catalyst Optimizer (logical â†’ physical plan)\n",
    "* Tungsten engine\n",
    "* Whole-stage code generation\n",
    "* Predicate pushdown\n",
    "* Column pruning"
   ]
  },
  {
   "cell_type": "code",
   "id": "6b00c28b",
   "metadata": {
    "language": "python",
    "ExecuteTime": {
     "end_time": "2026-02-02T12:15:33.735080100Z",
     "start_time": "2026-02-02T12:15:33.572672400Z"
    }
   },
   "source": "# df.explain(mode=\"formatted\")",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "1538f63e",
   "metadata": {},
   "source": "You should **read execution plans fluently**.\n"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "## Step 4: Partitioning & Shuffles (Performance Breakpoint)\n",
    "\n",
    "This is where **10x performance gains** happen.\n",
    "\n",
    "### Key Topics\n",
    "\n",
    "* What triggers a shuffle\n",
    "* Default partitions (`spark.sql.shuffle.partitions`)\n",
    "* Repartition vs Coalesce\n",
    "* Skewed data handling\n",
    "* Salting technique"
   ],
   "id": "a9884993b1793f9a"
  },
  {
   "cell_type": "code",
   "id": "9abb5936",
   "metadata": {
    "language": "python",
    "ExecuteTime": {
     "end_time": "2026-02-02T12:15:33.843919500Z",
     "start_time": "2026-02-02T12:15:33.765784100Z"
    }
   },
   "source": [
    "# df.repartition(200, \"country\")\n",
    "# df.coalesce(50)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "505345c2",
   "metadata": {},
   "source": [
    "\n",
    "### Rule of Thumb\n",
    "\n",
    "* CPU-bound â†’ more partitions\n",
    "* IO-bound â†’ fewer partitions\n",
    "* Never trust defaults blindly\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "## Step 5: Joins Mastery (Interview + Production)\n",
    "\n",
    "Most Spark failures come from joins.\n",
    "\n",
    "### Learn These Cold\n",
    "\n",
    "* Broadcast joins\n",
    "* Sort-merge joins\n",
    "* Shuffle hash joins\n",
    "* Join reordering"
   ],
   "id": "1457d826aa1a275e"
  },
  {
   "cell_type": "code",
   "id": "48cb0b72",
   "metadata": {
    "language": "python",
    "ExecuteTime": {
     "end_time": "2026-02-02T12:15:33.971163400Z",
     "start_time": "2026-02-02T12:15:33.852302900Z"
    }
   },
   "source": [
    "# from pyspark.sql.functions import broadcast\n",
    "#\n",
    "# df_large.join(broadcast(df_small), \"id\")"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "cf807eb3",
   "metadata": {},
   "source": [
    "### Pro Tip\n",
    "\n",
    "Always ask:\n",
    "\n",
    "> Which side is small?\n",
    "> Is broadcast safe?\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "## Step 6: Memory Management & Tuning\n",
    "\n",
    "You are now entering **senior-level PySpark**.\n",
    "\n",
    "### Spark Memory Areas\n",
    "\n",
    "* Execution memory\n",
    "* Storage memory\n",
    "* Off-heap memory\n",
    "\n",
    "### Critical Configs"
   ],
   "id": "1ab26d0dd3ebba54"
  },
  {
   "cell_type": "code",
   "id": "356bbbfe",
   "metadata": {
    "language": "text",
    "ExecuteTime": {
     "end_time": "2026-02-02T12:15:34.021520800Z",
     "start_time": "2026-02-02T12:15:33.971163400Z"
    }
   },
   "source": [
    "# spark.executor.memory\n",
    "# spark.executor.cores\n",
    "# spark.sql.adaptive.enabled\n",
    "# spark.sql.autoBroadcastJoinThreshold"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "a6fa86c2",
   "metadata": {},
   "source": [
    "You must understand:\n",
    "\n",
    "* OOM errors\n",
    "* GC overhead\n",
    "* Spill to disk\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "## Step 7: File Formats & Data Layout\n",
    "\n",
    "Storage layout = performance.\n",
    "\n",
    "### Pro Choices\n",
    "\n",
    "* Prefer **Parquet**\n",
    "* Use partitioned tables\n",
    "* Bucketing for joins"
   ],
   "id": "fa4fedd960f4cfb8"
  },
  {
   "cell_type": "code",
   "id": "58b0ac50",
   "metadata": {
    "language": "python",
    "ExecuteTime": {
     "end_time": "2026-02-02T12:15:34.703569900Z",
     "start_time": "2026-02-02T12:15:34.054866700Z"
    }
   },
   "source": [
    "df.write \\\n",
    "  .partitionBy(\"year\", \"month\") \\\n",
    "  .format(\"parquet\") \\\n",
    "  .save(path)"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43mdf\u001B[49m.write \\\n\u001B[32m      2\u001B[39m   .partitionBy(\u001B[33m\"\u001B[39m\u001B[33myear\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mmonth\u001B[39m\u001B[33m\"\u001B[39m) \\\n\u001B[32m      3\u001B[39m   .format(\u001B[33m\"\u001B[39m\u001B[33mparquet\u001B[39m\u001B[33m\"\u001B[39m) \\\n\u001B[32m      4\u001B[39m   .save(path)\n",
      "\u001B[31mNameError\u001B[39m: name 'df' is not defined"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "c645efe9",
   "metadata": {},
   "source": [
    "Learn:\n",
    "\n",
    "* Small file problem\n",
    "* Compaction strategies\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 8: PySpark on Cloud (REAL WORLD)\n",
    "\n",
    "PySpark without cloud is incomplete.\n",
    "\n",
    "### AWS / Azure / GCP\n",
    "\n",
    "* Spark on EMR / Databricks\n",
    "* S3 vs HDFS behavior\n",
    "* IAM roles & permissions\n",
    "* Cost optimization\n",
    "\n",
    "### Production Concerns\n",
    "\n",
    "* Retry handling\n",
    "* Idempotent pipelines\n",
    "* Checkpointing\n"
   ],
   "id": "608c8475a9cce32c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "## Step 9: Debugging & Monitoring\n",
    "\n",
    "A pro **debug fast**.\n",
    "\n",
    "### Tools\n",
    "\n",
    "* Spark UI (Stages, SQL, Storage)\n",
    "* Logs (driver & executor)\n",
    "* `df.explain()`\n",
    "\n",
    "![Image](https://spark.apache.org/docs/latest/img/AllStagesPageDetail3.png)\n",
    "\n",
    "![Image](https://assets.datacamp.com/production/repositories/3937/datasets/f184a047dcdcdcac854685bac07e571406e4ff09/SparkUI_count.png)\n",
    "\n",
    "\n",
    "You should diagnose:\n",
    "\n",
    "* Slow stages\n",
    "* Skew\n",
    "* Executor loss\n"
   ],
   "id": "e84fd3b3dbc9a425"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Step 10: Advanced Topics (Elite Level)\n",
    "\n",
    "Now youâ€™re in **top 10% PySpark engineers**.\n",
    "\n",
    "### Must Learn\n",
    "\n",
    "* Adaptive Query Execution (AQE)\n",
    "* Delta Lake (ACID on Spark)\n",
    "* Structured Streaming\n",
    "* Watermarking\n",
    "* Exactly-once semantics\n"
   ],
   "id": "2ff42c662e62176"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "## Step 11: Real Projects (NON-NEGOTIABLE)\n",
    "\n",
    "Theory without projects = zero value.\n",
    "\n",
    "### Project Ideas\n",
    "\n",
    "1. **Clickstream analytics pipeline**\n",
    "2. **SCD Type-2 ETL with Spark**\n",
    "3. **Real-time streaming with Kafka + Spark**\n",
    "4. **Cost-optimized Spark job on S3**\n",
    "5. **Data quality framework using Spark**\n"
   ],
   "id": "9d1df4f485c48ee8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "## Step 12: Interview Readiness (Pro Signal)\n",
    "\n",
    "You must explain:\n",
    "\n",
    "* Why Spark over Pandas\n",
    "* How shuffle works\n",
    "* How to fix skew\n",
    "* How to reduce job runtime by 50%\n"
   ],
   "id": "1deeba1683553cac"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "## Daily Practice Plan (Pro Schedule)\n",
    "\n",
    "| Time   | Activity             |\n",
    "| ------ | -------------------- |\n",
    "| 30 min | Read execution plans |\n",
    "| 45 min | Code transformations |\n",
    "| 30 min | Performance tuning   |\n",
    "| 15 min | Spark UI analysis    |\n"
   ],
   "id": "5ae2465a39bbd290"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "## Final Reality Check\n",
    "\n",
    "If you can:\n",
    "\n",
    "* Read Spark plans\n",
    "* Tune joins & partitions\n",
    "* Debug slow jobs\n",
    "* Design cloud-ready pipelines\n",
    "\n",
    "ðŸ‘‰ **You are a PySpark pro**"
   ],
   "id": "602f0bb798aa3264"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
